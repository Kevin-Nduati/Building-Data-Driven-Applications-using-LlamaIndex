{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents\n",
    "A document serves as a container. It holds not just the raw text or data from wherever it originated but also any extra bits of information you decide to tag along. Here os how it can be created manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: f3bf2752-ba8f-4e6b-8aae-2862cda096b9\n",
      "Text: The quick brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "doc = Document(\n",
    " text = text,\n",
    " metadata = {\"author\": \"John Doe\", \"category\": \"others\"},\n",
    " id=\"1\"\n",
    ")\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical applications, these documents are generated in bulk by sourcing them from various data sources. This bulk ingestion of data uses predefined data loaders - sometimes called connectors  or simply readers- from an extensive library called Llamahub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(\n",
    "    pages = {'Pythagorean theorem', 'General relativity'}\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating documents is a straightforward process. But how do the raw document objects get converted into a format that LLMs can efficiently process and reason over? This is here Nodes come in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes\n",
    "While Documenst represent the raw data and can be used as such, Nodes are smaller chunks of content extracted from the documents. The goal is to break down documents into smaller, more manageable pieces of text. This serves the following purposes:\n",
    "* Allow our proprietary knowledge to fit within the model's prompt limits\n",
    "* Allows the creation of relationships between Nodes\n",
    "\n",
    "In LlamaIndex, nodes can also store images. \n",
    "Here is a list of attributes of the TextNode Class:\n",
    "* **text**: The chunk of text derived from an original document\n",
    "* **start_char_idx** and **end_char_idx** are optional index integer values that store the starting and ending character positions of the text within the Document. This could be helpful when the text is part of a larger document, and you need to pin point the exact location\n",
    "* **text_template** and **metadata_template** are template fields that define how the text and metadata are formatted. They help produce a more structured and readable representation of TextNode\n",
    "* **metadata_separator**: This is a string field that defines the separator between metadata fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually creating the Node Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: ea929b60-7c90-4c21-8d4a-db3414a8ca92\n",
      "Text: This is a sample\n",
      "Node ID: c540d961-ea1c-413e-911a-7a84f3f8fcac\n",
      "Text: Document text\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "doc = Document(text=\"This is a sample Document text\")\n",
    "n1 = TextNode(text = doc.text[0:16], doc_id=doc.id_)\n",
    "n2 = TextNode(text=doc.text[17:30], doc_id=doc.id_)\n",
    "\n",
    "\n",
    "print(n1)\n",
    "print(n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically extracting nodes from documents using splitters\n",
    "Because Document chunking is very important in a RAG workflow, LlamaIndex comes with built-in tools for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (6) is close to chunk size (12). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "This is sentence 1.\n",
      "{'author': 'John Smith'}\n",
      "This is sentence 2.\n",
      "{'author': 'John Smith'}\n",
      "Sentence 3 here\n",
      "{'author': 'John Smith'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "doc = Document(\n",
    "    text=(\"This is sentence 1. This is sentence 2. \" \n",
    "          \"Sentence 3 here\"),\n",
    "    metadata={\"author\": \"John Smith\"}\n",
    ")\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=12,\n",
    "    chunk_overlap=0,\n",
    "    separator=\" \"\n",
    ")\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents([doc])\n",
    "for node in nodes:\n",
    "    print(node.text)\n",
    "    print(node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes don't like to be alone- they crave relationships\n",
    "We can now add relationships to nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<NodeRelationship.NEXT: '3'>: '3d4aa0a8-1b23-42ef-a4cb-c9a2e6d906e7'}\n",
      "{<NodeRelationship.PREVIOUS: '2'>: 'd97af7bc-f8c0-41e4-b168-5ebb8b59e6cb'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "\n",
    "doc = Document(text=\"First sentence. Second sentence\")\n",
    "n1 = TextNode(text=\"First sentence\", node_id=doc.doc_id)\n",
    "n2 = TextNode(text=\"Second sentence\", node_id= doc.doc_id)\n",
    "\n",
    "n1.relationships[NodeRelationship.NEXT] = n2.node_id\n",
    "n2.relationships[NodeRelationship.PREVIOUS] = n1.node_id\n",
    "\n",
    "print(n1.relationships)\n",
    "print(n2.relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex contains the necessary tiiks to automatically create relationships between the nodes. For example, when using the automated node parsers discussed previously in ntheir default configuration, LlamaIndex will automatically create previous ir bext relationships between the nodes it creates\n",
    "\n",
    "There are other types of relationships that we could define. In addition to somple relationships such as previous or next, Nodes can be connected using the following:\n",
    "* **SOURCE-** The source relationship represents the original document that a node was extracted or parsed from. When you parse a document into multiple nodes, you can track which document each node originated from using the source relationship\n",
    "* **PARENT-** The parent relationship indicates a hierarchical structure where the node with this relationship is one level higher that the associated node, In a tree structure, a parent would have one or more children\n",
    "* **CHILDREN-** This is the opposite of parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are relationships important\n",
    "* **Enables more contextual querying-** By linking nodes together, you can leverage their relationships during querying to retrieve additional relevant documents\n",
    "* **Allows tracking provenance-** Relationships encode provenance- where source nodes originated and how they ate connected\n",
    "* **Enables navigation through nodes-** Traversing nodes by their relationships enables new types of queries\n",
    "* **Supports the construction of knowledge graphs-** Nodes and relationships are the building blocks of knowledge graphs\n",
    "* **Improves the index structure-** Some LlamaIndexes such as trees and graphs, utilize node relationships to build their internal structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes\n",
    "The index refers to a specific data structure used to organize a collection of nodes for optimized storage and retrieval\n",
    "Without indexing, your data is a messy pile of disorganized facts and files. Proper indexing neatly sorts information intop categories that make sense.\n",
    "LlamaIndex supports different types of indexes, each with its own strength and trade-offs. Here is a list of available index types:\n",
    "* **SummaryIndex-** This is a very similar to a box of recipes - it keeps your nodes in order so that you can access them one by one.\n",
    "* **DocumentSummaryIndex-** This constructs a concise summary for each document, mapping these summaries back to their respective nodes. \n",
    "* **VectorStoreIndex-** It converts text into vector embeddings and uses math to group similar nodes, helping you to locate nodes that are alike\n",
    "* **TreeIndex-** This index behaves similarly to putting smaller boxes inside bigger ones, organizing nodes in a tre-like structure\n",
    "* **KeywordTableIndex-** The keyword index connects important words to the nodes they are in\n",
    "* **KnowledgeGraphIndex-** This is useful when you need to link facts in a big network of data stored as a knowledge graph\n",
    "* **Composable Graph-** This allows you to create index structures in which Document-level indexes are indexed in higher-level collections\n",
    "\n",
    "\n",
    "All the index types in llamaIndex share some common features:\n",
    "* **Building the index-** Each index can be constructed by passing in a set of nodes during initialization. This builds the underlying index structure\n",
    "* **Inserting new nodes-** After an index is built, new nodes can be manually inserted. This adds to the existing index structure\n",
    "* **Querying the index-** Once built, indexes provide a query interface to retrieve relevant nodes based on a specific query\n",
    "\n",
    "Here is a simple exampel to illustrate the creation of SummaryIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, Document\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = [\n",
    "    TextNode(\n",
    "        text=\"Lionel Messi is a footbal player from Argentina\"\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"He has won Ballon d'or trophy 7 times\"\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Lionel Messi's hometown is Rosario\"\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"He was born on June 24, 1987\"\n",
    "    )\n",
    "]\n",
    "\n",
    "index = SummaryIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rosario\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is Messi's hometown?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QueryEngine contains a retriever, which nis responsible for retrieving Nodes from the index for the query. The retriever does a lookup to fetch and rank relevant nodes from the index for that query. It grabs nodes from the index that are likely ti contain information about Messi's hometown. \n",
    "But just getting back a list of nodes isn't very useful. Another part of QueryEngine called the node postprocessor comes into play at this point. This part enables the transformation, re-ranking, or fiktering of nodes after thy've been reytrieved and befire the final response is crafted.\n",
    "The QueryEngine object contains a response synthesizer, which takes the retrieved nodes and crafts the final response using the following steps:\n",
    "1. The response synthesizer takes the nodes selected by the retriever and processed by the node postprocessor and formats them into an LLM prompt.\n",
    "2. The prompt contains the query along with context from the nodes\n",
    "3. This prompt is given to the LLM to generate a response\n",
    "4. Any necessary postprocessing is done on the raw response using the LLM to return the final natural language amswer\n",
    "\n",
    "So index.as_query_engine() is creating a full query engine for us, containing a default version of the three elements: retriever, node postprocessor and response synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our first interactive, augmented LLM Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask me anything about Lionel Messi\n",
      "Lionel Messi was born on 24 June 1987.\n",
      "Inter Miami\n",
      "Inter Miami.\n",
      "Antonela Roccuzzo is Messi's wife, and they have three sons named Thiago, Mateo, and Ciro.\n",
      "Antonela Roccuzzo is Messi's wife, and they have three sons named Thiago, Mateo, and Ciro.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document, SummaryIndex\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=[\"Messi Lionel\"])\n",
    "parser = SimpleNodeParser.from_defaults()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "index = SummaryIndex(nodes)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "print(\"Ask me anything about Lionel Messi\")\n",
    "while True:\n",
    "    question = input(\"Your Question: \")\n",
    "    if question.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    response = query_engine.query(question)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the logging features of LlamaIndex to understand the logic and debug our applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Parameter\n",
    "The temperature values for the OpenAI values range from 0 to 2. Higher values produce more random, creative output. Lower values produce more focused, deterministic output. A temperature of 0 will produce almost the same output every time for the same input prompt. For code generation and data analysis tasks, a temperature value of 0.2 would be appropriate, while kore creative-focused tasks such as writing or chatbot responses will benefit from a setting of 0.5 or higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding how Settings can be used for customization\n",
    "Settings is a key component in LlamaIndex that allows you to customize and configure the elements used during indexing and querying. It contains common objects needed across LlamaIndex such as the following:\n",
    "* **LLM-** This allows for the overriding of the default LLM witrh a custom one as we've seen\n",
    "* **Embedding model-** This is used for generating vectors for text to enable semantic search\n",
    "* **NodeParser-** This is used for setting the default node parser\n",
    "* **CallbackManager-** This handles callbacks for events within LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
